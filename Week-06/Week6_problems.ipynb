{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ye5YFtvToNe"
   },
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 7 (Model Interpretation and Feature selection)\n",
    "\n",
    "\n",
    "## Learning goals ðŸŽ¯\n",
    "1. Learn how to use different strategies for interpreting machine learning models.\n",
    "2. Learn how to properly implement feature selection to avoid leaking information.\n",
    "\n",
    "### Acknowledgements\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.6.1 matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Zp3mpVv4A9_W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jbHJ5b5A9_Y"
   },
   "source": [
    "# Part 1: A common error: leaking information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AldmQ19uA9_Y"
   },
   "source": [
    "We will start with a toy example to illustrate a common mistake when using feature selection. We will create a random dataset with 10.000 features and 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MAm-RPVBA9_Z"
   },
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "X_test = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))\n",
    "y_test = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JPA5RNRA9_Z",
    "outputId": "7db65bb3-ab34-4925-9408-7cdcadf70126"
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC11ZosYA9_a"
   },
   "source": [
    "We might consider that 10.000 is a very high number of features and that we need to use feature selection. So, let's select the 5% most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT3sN3KeA9_a",
    "outputId": "b3936f72-1fe1-4abb-9278-20f8932d7543"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "select = SelectPercentile(score_func=f_regression,\n",
    "                          percentile=5)\n",
    "select.fit(X, y)\n",
    "X_sel = select.transform(X)\n",
    "\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlJXXsIeA9_b"
   },
   "source": [
    "Now we will create a pipeline to pre-process the data and fit a regression model to see if we can predict the random labels from the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSntGdZ8A9_b",
    "outputId": "f55f77ff-6be4-4bb2-bade-0b79f3fb66fe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, random_state=0)\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tas54WLSA9_c"
   },
   "source": [
    "These are great results but how did we get such good results on a random dataset?\n",
    "\n",
    "These results are due to information leaking as the features were selected before spliting the data into train and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk97Om8OA9_c"
   },
   "source": [
    "### Task 1: Implement a correct pipeline to pre-process the data, select the top 5% features and train a regression model to predict th random labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxqz_phZA9_c",
    "outputId": "88c78ea7-53ec-40bf-8819-99232e03eeea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pCcvR8DA9_d"
   },
   "source": [
    "These results make more sense from what we would expet with random labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBrEYQUbA9_d"
   },
   "source": [
    "# Part 2: Model interpretation and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1_HOv7dA9_e"
   },
   "source": [
    "## Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC)\n",
    "\n",
    "For this part, we will use data from the **Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC)**.\n",
    "\n",
    "**Source:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)  \n",
    "\n",
    "**Samples:** 569 (357 Benign, 212 Malignant)  \n",
    "\n",
    "**Target Variable:** Diagnosis (**M** = Malignant, **B** = Benign)  \n",
    "\n",
    "### Features (30 total)\n",
    "- **10 Cell Nucleus Characteristics**, including:\n",
    "  - Radius, Texture, Perimeter, Area, Smoothness, Compactness, Concavity, Concave Points, Symmetry, Fractal Dimension  \n",
    "- Each feature has **Mean, Standard Error (SE), and Worst** (largest mean of top 3 values) variations  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "2bbrRMTOToNk",
    "outputId": "125a2d2b-3b5c-4f83-9d2d-800acee0df3d"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['diagnosis'] = data.target  # 0 = Benign, 1 = Malignant\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssZKhHxZToNk"
   },
   "source": [
    "Now we identify features X and targets y. The column WAGE is our target variable (i.e., the variable which we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Vp3beQbEToNl",
    "outputId": "7fda36ea-481f-4235-dbc2-3fd2b11c7e28",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"diagnosis\"])  # Exclude non-feature columns\n",
    "y = df[\"diagnosis\"]  # Target variable (M = Malignant, B = Benign)\n",
    "\n",
    "# Display summary statistics\n",
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "44iBUDR6ToNm",
    "outputId": "0a0be4b8-a2fe-4070-ab2c-adc9d3fc48c4"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgOH32mFToNm"
   },
   "source": [
    "Our target for prediction: Diagnosis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "KEv3tul5ToNm",
    "outputId": "589a6081-5ba2-459b-f930-9f3191cde0ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the target variable (y)\n",
    "y = df[\"diagnosis\"].values.ravel()\n",
    "\n",
    "# Display the first few values\n",
    "df[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtFe0csGToNn"
   },
   "source": [
    "We now split the sample into a train and a test dataset. Only the train dataset will be used in the following exploratory analysis. This is a way to emulate a real situation where predictions are performed on an unknown target, and we donâ€™t want our analysis and decisions to be biased by our knowledge of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IajsRtoNToNn"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgEMsTCKToNo"
   },
   "source": [
    "First, letâ€™s get some insights by looking at the a matrix showing the correlation of all features with each other between them. Only numerical variables will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zY0615zHToNo",
    "outputId": "80d45fe5-f857-42e0-e382-5c9c1a881916"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe6HBlS8ToNp"
   },
   "source": [
    "We can see that several features are strongly correlated. For example, \"mean radius\", \"mean perimeter\" and \"mean area\" are very strongly correlated with each other. They are also correlated to all other features in the same way. This indicates that these 3 features provide the same or very similar information about the tumor shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr3AqplYToNp"
   },
   "source": [
    "Before designing a machine learning pipeline, we should check the type of data that we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_maegsSToNp",
    "outputId": "c581f4ca-45db-4f30-adc3-6c525610d4de"
   },
   "outputs": [],
   "source": [
    "# Check dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1ljuZ0oToNp"
   },
   "source": [
    "All features are numerical and unbounded, suggesting we should scale all of them before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt-dOJIVToNq"
   },
   "source": [
    "## Task 2: Machine Learning Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Implement a **machine learning pipeline** that includes **preprocessing and cross-validation** to optimize the model's hyperparameters. \n",
    "- Use the pipeline with **linear SVM** and **regularized logistic regression with L1 and elastic-net regularization** to predict whether a tumor is **malignant or benign** based on the given features. \n",
    "- Create a table to show the performance of the different models. \n",
    "- Plot the confusion matrix and ROC curve for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "8GSF28gHToNq",
    "outputId": "670e47f1-20d1-4ebb-f3d8-81c0683e4781"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Preprocessing: Standardize numerical features\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), X.columns),  # Standardize all features\n",
    "    verbose_feature_names_out=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training models\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "def train_cv(model, param_grid):\n",
    "    preprocess_and_train = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"classify\", model)\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=preprocess_and_train,\n",
    "        param_grid=param_grid,\n",
    "        n_jobs=-1,\n",
    "        error_score=0,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    return grid_search.fit(X_train, y_train) \n",
    "\n",
    "# defining parameter range \n",
    "cv_svc = train_cv(\n",
    "    LinearSVC(dual = \"auto\", random_state=42),\n",
    "    {'classify__C': [0.1, 1,]}\n",
    ")\n",
    "model_svc=cv_svc.best_estimator_\n",
    "\n",
    "cv_lasso = train_cv(\n",
    "    LogisticRegression(\n",
    "        penalty=\"l1\",  # Lasso (L1 regularization)\n",
    "        solver=\"liblinear\",  # Required for L1 penalty\n",
    "        max_iter=100000,\n",
    "    ),\n",
    "    {'classify__C': np.logspace(-3, 3, 10)}\n",
    ")\n",
    "model_Lasso = cv_lasso.best_estimator_\n",
    "\n",
    "cv_en = train_cv(\n",
    "    LogisticRegression(\n",
    "        penalty=\"elasticnet\",  # Lasso (L1 regularization)\n",
    "        solver=\"saga\",  # Required for L1 penalty\n",
    "        max_iter=100000,\n",
    "    ),\n",
    "    {'classify__C': np.logspace(-3, 3, 10), \"classify__l1_ratio\": [0.1, 0.5, 0.9]}\n",
    ")\n",
    "model_EN = cv_en.best_estimator_\n",
    "\n",
    "print(\"Done training models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# Evaluate models and plot confusion matrices and ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lvV0O5eToNr"
   },
   "source": [
    "### Task 2.2 Plot the models coefficients variability across folds for the linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxqBSUy4A9_p"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwJR4MyGToNr"
   },
   "source": [
    "Discussion: Are the coefficents across the different models similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkTgdVizToNr"
   },
   "source": [
    "### Task 2.3 Plot the permutation feature importance for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "lOd9aFdcHNva",
    "outputId": "4cebceed-529b-419a-d593-c28c8d2ff608"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycu4ELxkToNs"
   },
   "source": [
    "Discussion: Are the feature coefficients simimar to the permutation importance for the different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoX5Z_1FToNs"
   },
   "source": [
    "### Task 2.4 Implement a similar pipeline for tree-based models and use the pipeline with Random Forest and Gradient Boosting trees to predict the tumour malignancy from the other features.\n",
    "- Create a table to show the performance of the different models. \n",
    "- Plot the confusion matrix and ROC curve for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "LV-pOg9eIThB",
    "outputId": "8da23915-f80a-48c0-c278-34364229c6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training models\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(max_depth=4, random_state=0, n_estimators=100, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit Random Forest model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gb_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    GradientBoostingClassifier(max_depth=4, random_state=0, n_estimators=100)\n",
    ")\n",
    "\n",
    "# Fit Gradient Boosting model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# These are the pre-trained models that you can use for the next sections\n",
    "tree_models = [\n",
    "    (\"Random Forest\", rf_model),\n",
    "    (\"Gradient Boosting\", gb_model)\n",
    "]\n",
    "\n",
    "print(\"Done training models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kKckTHiToNs"
   },
   "source": [
    "### Task 2.5 Plot the feature importance for the different tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "yKhZrLtfhAQD",
    "outputId": "ed51e7fb-e4dd-4c09-b1f5-f224b97e66f3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7O5LF5NToNs"
   },
   "source": [
    "### Task 2.6 Plot the permutation feature importance for the different tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "oFDzDpLFToNs",
    "outputId": "727fd1c2-59bc-4880-ff33-006d7b17a395"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVTTWpnUToNs"
   },
   "source": [
    "Discussion: Are the feature importance and permutation feature importance similar for the different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFKo9mhlToNt"
   },
   "source": [
    "### Task 2.7  For the best tree-based model use partial dependence plot to investigate dependence between the target response and each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "wXO8NBfFQzzL",
    "outputId": "654deea5-8a7d-41e2-8be1-fbee313bb8d5"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL4jXNqvToNt"
   },
   "source": [
    "## Task 3: Include feature selection within the cross-validation pipeline implemented in Task 1 and try two different feature selection strategies (select k best and recursive feature elimination) with the linear SVM model.\n",
    "- Create a table to show the performance of the different models. \n",
    "- Plot the confusion matrix and ROC curve for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAAlj1PRToNt"
   },
   "source": [
    "Discussion: Did the model performance improved with feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-J27zzgToNu"
   },
   "source": [
    "### Task 3.2 Plot the coefficientes variability across folds for the linear model based on the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "kpewjP15ToNu",
    "outputId": "9aa5dd4d-b34d-46f8-a1f8-5b751f817435"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haLNCVvgToNu"
   },
   "source": [
    "Discussion: Are similar features selected using the different strategies?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
