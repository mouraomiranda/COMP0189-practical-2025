{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kimdanny/COMP0189-practical/blob/main/Week-08/week8-problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QvZjsta-6r8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 8 (Reinforcement Learning)\n",
    "\n",
    "In this notebook you will learn how to implement from scratch the Q-Learning algorithm for deterministic Markov\n",
    "Decision Process and how to use the Deep Q-Learning algorithm using the Pytorch library.\n",
    "\n",
    "Imagine we want to train an impostor from\n",
    "[Among Us](https://en.wikipedia.org/wiki/Among_Us) to make it capable of finding the closest vent to\n",
    "hide himself. To simplify this task we assume that the impostor moves in a grid world.\n",
    "\n",
    "We can model the impostor in this world as an agent that implements 4 actions,\n",
    "move up, move down, move right and move left,\n",
    "and we can model the environment as a grid world where the agent gets rewarded only once it has achieved its goal,\n",
    "it has found the vent.\n",
    "\n",
    "## The Environment\n",
    "\n",
    "If in supervised learning, we would usually spend most of our time collecting and cleaning the data, in RL this\n",
    "time is spent in designing the environment. Following we design a `GridWorld` class.\n",
    "\n",
    "### Acknowledgements\n",
    "This notebook is adapted from [CEGE0004: Machine Learning for Data Science](https://github.com/aldolipani/CEGE0004) (week 8 RL practical), written by [Dr. Aldo Lipani](https://aldolipani.com) (aldo.lipani@ucl.ac.uk), an Asst. Prof. in Machine Learning at the University College London (UCL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lg0zeKaTABgu"
   },
   "outputs": [],
   "source": [
    "# Getting the Among Us images\n",
    "! wget https://raw.githubusercontent.com/kimdanny/COMP0189-practical/main/Week-08/imgs/agent.png\n",
    "! wget https://raw.githubusercontent.com/kimdanny/COMP0189-practical/main/Week-08/imgs/target.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TVlwZWe-6r-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "class GridWold:\n",
    "\n",
    "    def __init__(self, n, init_pos=(0, 0), target_pos=None):\n",
    "        \"\"\"\n",
    "        A simple environment\n",
    "        :param n: The size of the world.\n",
    "        :param init_pos: The initial position of the agent. The agent start at the top-left corner by default.\n",
    "        :param target_pos: The position of the goal. The goals is set to the bottom-right corner by default.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.init_pos = init_pos\n",
    "        self.pos = init_pos\n",
    "        if target_pos is None:\n",
    "            self.target_pos = (n - 1, n - 1)\n",
    "        else:\n",
    "            self.target_pos = target_pos\n",
    "\n",
    "    def reset(self, pos=None):\n",
    "        \"\"\"\n",
    "        This method resets the environment.\n",
    "        :param pos: If we set the position, the agent will start not from the default position by from the\n",
    "        position set. This is useful when we will add some randomness in the training.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if pos is None:\n",
    "            self.pos = self.init_pos\n",
    "        else:\n",
    "            self.pos = pos\n",
    "        return self.pos\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        \"\"\"\n",
    "        This method returns the set of available actions the agent can perform given its current position.\n",
    "        :return: The set of available actions.\n",
    "        \"\"\"\n",
    "        # if the agent has reached the vent, then no actions are available.\n",
    "        if self.pos == self.target_pos:\n",
    "            res = {}\n",
    "        else:\n",
    "            # all actions\n",
    "            res = {'up', 'down', 'right', 'left'}\n",
    "            # remove actions if the agent is at the edges of the world.\n",
    "            if self.pos[0] == 0:\n",
    "                res.discard('left')\n",
    "            elif self.pos[0] == self.n - 1:\n",
    "                res.discard('right')\n",
    "            if self.pos[1] == 0:\n",
    "                res.discard('up')\n",
    "            elif self.pos[1] == self.n - 1:\n",
    "                res.discard('down')\n",
    "        return res\n",
    "\n",
    "    def step(self, action: str):\n",
    "        \"\"\"\n",
    "        This method executes an action and changes the state of the agent. It returns the new position of the agent and\n",
    "        the reward received.\n",
    "        :param action: The action to execute.\n",
    "        :return: The position and reward.\n",
    "        \"\"\"\n",
    "        # executes an action\n",
    "        if action == 'left':\n",
    "            self.pos = (self.pos[0] - 1, self.pos[1])\n",
    "        elif action == 'right':\n",
    "            self.pos = (self.pos[0] + 1, self.pos[1])\n",
    "        elif action == 'down':\n",
    "            self.pos = (self.pos[0], self.pos[1] + 1)\n",
    "        elif action == 'up':\n",
    "            self.pos = (self.pos[0], self.pos[1] - 1)\n",
    "\n",
    "        # if the agent has achieved the goal, it returns a reward of 100, zero otherwise.\n",
    "        if self.pos == self.target_pos:\n",
    "            return self.pos, 100\n",
    "        else:\n",
    "            return self.pos, 0\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        This method renders the environment and the agent.\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        agent = OffsetImage(plt.imread('agent.png'), zoom=1 / (2 * n))\n",
    "        target = OffsetImage(plt.imread('target.png'), zoom=10 / (2 * n))\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        for j in range(n + 1):\n",
    "            ax.axvline(x=j / n)\n",
    "            ax.axhline(y=j / n)\n",
    "\n",
    "        ab_target = AnnotationBbox(target,\n",
    "                                   xy=((self.target_pos[0] + 0.5) / n, 1 - (self.target_pos[1] + 0.7) / n),\n",
    "                                   frameon=False)\n",
    "        ax.add_artist(ab_target)\n",
    "\n",
    "        ab_agent = AnnotationBbox(agent,\n",
    "                                  xy=((self.pos[0] + 0.5) / n, 1 - (self.pos[1] + 0.5) / n),\n",
    "                                  frameon=False)\n",
    "        ax.add_artist(ab_agent)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl5gMfOz-6r_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now instantiate a gird-world of size 5 and render it. If we use the default parameters, you should see the\n",
    "impostor on the top-left corner, and the vent on the bottom-right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lL3x16qq-6sA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = GridWold(5)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TipWcjx-6sA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# A Random Policy\n",
    "\n",
    "We will now simulate a random policy. An agent that navigates the world performing random actions. This agent will\n",
    "execute actions for a given number of times. While executing the actions\n",
    "we will also count the number of times the agent has achieved its goal (the number of episodes).\n",
    "\n",
    "## Overview\n",
    "A Random Policy is a **basic decision-making strategy** in reinforcement learning where an agent selects actions uniformly at random. This approach serves as:\n",
    "\n",
    "- Baseline comparison for learning algorithms\n",
    "- Pure exploration mechanism\n",
    "\n",
    "- Simple starting point for policy analysis\n",
    "\n",
    "\n",
    "## Mathematical Formulation\n",
    "For discrete action space with $n$ possible actions:\n",
    "$$\\pi(a|s) = \\frac{1}{n}\\ \\ \\forall a \\in \\mathcal{A}(s)$$\n",
    "\n",
    "Note that we are slowing down the execution of this policy to make this policy visible; We add a 1ms delay at every\n",
    "agent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0JGhA-PuQw-"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9zmj5pRuQw_"
   },
   "source": [
    "**Task 1. Run your random policy and render the image each step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1ZRsxAm-6sB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the number of actions the agent will execute in total\n",
    "total_steps = 1000\n",
    "\n",
    "episodes = 0\n",
    "for n in tqdm(range(total_steps)):\n",
    "    # delay for display\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    available_actions = env.get_available_actions()\n",
    "    # Implement and run a random policy\n",
    "    ### YOUR CODE GOES BELOW ###\n",
    "    None\n",
    "    \n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTrG9ieZ-6sB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q-Learning\n",
    "\n",
    "We will now implement the Q-learning algorithm to learn the Q-values.\n",
    "\n",
    "## Overview\n",
    "Q-Learning is a **model-free reinforcement learning** algorithm that enables agents to learn optimal policies through interaction with an environment. It uses a Q-table to store expected future rewards for each state-action pair $(s,a)$.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Q-Table**: Matrix storing $Q(s,a)$ values\n",
    "- **Learning Rate ($\\alpha$)**: Controls update magnitude (0 < α ≤ 1)\n",
    "\n",
    "- **Discount Factor ($\\gamma$)**: Values future rewards (0 ≤ γ ≤ 1)\n",
    "- **Epsilon-Greedy**: Balances exploration/exploitation\n",
    "\n",
    "## Bellman Update Equation\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha\\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right]$$\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "1. Initialize Q-table with zeros\n",
    "2. Observe current state $s$\n",
    "\n",
    "3. Choose action $a$ (epsilon-greedy)\n",
    "4. Receive reward $r$, observe new state $s'$\n",
    "\n",
    "5. Update Q-value using Bellman equation\n",
    "6. Repeat until convergence\n",
    "\n",
    "## Instruction\n",
    "In the following code, we simplify the equation by setting the Learning Rate ($\\alpha$) to 1, therefore $$Q(s,a) \\leftarrow r + \\gamma \\max_{a'} Q(s',a')$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCMb3NaOuQw_"
   },
   "source": [
    "**Task 2. Implement and train with Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1T3SpMA-6sB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# discount rate\n",
    "l = 0.9\n",
    "# the number of actions the agent will execute in total\n",
    "total_steps = 1000\n",
    "# q-values\n",
    "q_table = {}\n",
    "\n",
    "# reset env\n",
    "pos = env.reset()\n",
    "\n",
    "# render env\n",
    "env.render()\n",
    "time.sleep(0.1)\n",
    "display.clear_output(wait=True)\n",
    "\n",
    "episodes = 0\n",
    "for n in tqdm(range(total_steps)):\n",
    "    # delay for display\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    # get available actions\n",
    "    available_actions = env.get_available_actions()\n",
    "    # Implement and run Q-Learning\n",
    "    ### YOUR CODE GOES BELOW ###\n",
    "    None\n",
    "    \n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEOUKt1P-6sB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Task 3. Now that we have trained the agent we can test it. Compare the performance of this agent against the random policy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egKHze3B-6sB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the number of actions the agent will execute in total\n",
    "total_steps = 1000\n",
    "\n",
    "# reset env\n",
    "pos = env.reset()\n",
    "for n in tqdm(range(total_steps)):\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    # get available actions\n",
    "    available_actions = env.get_available_actions()\n",
    "    ### YOUR CODE GOES BELOW ###\n",
    "    None\n",
    "\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJSS74L1-6sC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## An example of Ready-Made Environments (MiniGrid)\n",
    "\n",
    "Here I provide you with an example of a ready-made environment where you can test RL algorithms.\n",
    "These environments are defined by the [MiniGrid](https://github.com/maximecb/gym-minigrid).\n",
    "\n",
    "Before executing the next code, please make sure that this package is correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGvbVr6t-6sC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym-minigrid==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7heuyKTQSP0"
   },
   "outputs": [],
   "source": [
    "from gym_minigrid.wrappers import *\n",
    "\n",
    "env = gym.make('MiniGrid-Empty-16x16-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsaXx6T9-6sC"
   },
   "source": [
    "This is how the environment looks like at the beginning. We have a grid world with an agent in the top-left corner\n",
    "that can navigate this empty room. On the bottom-right corner we have the target location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRtZWV1E-6sD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You might have to run this cell twice if you face error.\n",
    "plt.figure(figsize=(9, 9))\n",
    "env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJfNqjhZ-6sD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can access the actions available to the agent as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgnnS-xa-6sD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for action in list(env.actions):\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqw_YJuQ-6sD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You might have to run this cell twice if you face error.\n",
    "\n",
    "from tqdm import tqdm\n",
    "total_steps = 100\n",
    "\n",
    "for n in tqdm(range(total_steps)):\n",
    "    time.sleep(0.1)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, _, _ = env.step(action)\n",
    "\n",
    "    print(f'Iteration: {n + 1} of Episodes {episodes}')\n",
    "    print(f'Action taken: {action}')\n",
    "    print(f'Reward: {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2Pz1r3ii5Qs"
   },
   "source": [
    "## The Same Cartpole Example in Pytorch\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "[Run in Colab](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/9da0471a9eeb2351a488cd4b44fc6bbf/reinforcement_q_learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltmdbtUnj4SC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
