{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4660a734",
      "metadata": {
        "id": "4660a734"
      },
      "source": [
        "# COMP0189: Applied Artificial Intelligence\n",
        "## Week 9 (Clustering)\n",
        "\n",
        "\n",
        "### ðŸŽ¯ Objectives\n",
        "1. To learn how to apply different clustering approaches (K-Means, Hierarchical clustering and Gaussian Mixture Models) to different tasks: clustering images and clustering voxels (image segmentation)\n",
        "2. To learn how to quantify clustering results\n",
        "\n",
        "\n",
        "\n",
        "### Acknowledgements\n",
        "- Many thanks to Prof. John Ashburner from the Wellcome Center for Human Neuroimaging (UCL) for kindly providing the brain imaging data.\n",
        "- https://scikit-learn.org/stable/\n",
        "- https://en.wikipedia.org/wiki/MNIST_database\n",
        "- https://brain-development.org/ixi-dataset/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f55e3b",
      "metadata": {
        "id": "25f55e3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from collections import defaultdict\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b18ed1",
      "metadata": {
        "id": "11b18ed1"
      },
      "source": [
        "# Part 1: MNIST dataset\n",
        "\n",
        "In this part we will apply Principal Component Analysis (PCA) to the MNIST dataset and use K-Means to cluster the digits.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/clustering.html#k-means"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8026c4",
      "metadata": {
        "id": "fc8026c4"
      },
      "source": [
        "### Task 1: Load MNIST data and assemble it in two matrices X (images) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c691c6a",
      "metadata": {
        "id": "7c691c6a"
      },
      "outputs": [],
      "source": [
        "MNIST = np.load(\"mnist.npz\")\n",
        "for k in MNIST.files:\n",
        "    print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc88b6a",
      "metadata": {
        "id": "fdc88b6a"
      },
      "outputs": [],
      "source": [
        "MNIST[\"X\"].shape, MNIST[\"y\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a136cfd8",
      "metadata": {
        "id": "a136cfd8"
      },
      "outputs": [],
      "source": [
        "mnist_X = MNIST[\"X\"]\n",
        "mnist_y = MNIST[\"y\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fa23e5",
      "metadata": {
        "id": "07fa23e5"
      },
      "source": [
        "### Task 2: Visualise the data for better understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd01645",
      "metadata": {
        "id": "2bd01645"
      },
      "outputs": [],
      "source": [
        "def make_img_grid(images, n_cols=10):\n",
        "    \"\"\"Helper function for arranging images into a grid\"\"\"\n",
        "    cols = []\n",
        "    gap = len(images) % n_cols\n",
        "    if gap > 0:\n",
        "        # add padding if needed\n",
        "        images = np.concatenate(\n",
        "            (images, np.zeros((n_cols - gap,) + images[0].shape)), 0\n",
        "        )\n",
        "    for n in range(n_cols):\n",
        "        cols.append(np.concatenate(images[np.arange(n, len(images), step=n_cols)]))\n",
        "    return np.concatenate(cols, -1)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.imshow(make_img_grid(mnist_X[:300], n_cols=30), cmap=\"binary_r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36617b00",
      "metadata": {
        "id": "36617b00"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
        "plt.gray()\n",
        "\n",
        "# loop through subplots and add mnist images\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(mnist_X[i])\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Number {}\".format(mnist_y[i]))\n",
        "\n",
        "# display the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea624926",
      "metadata": {
        "id": "ea624926"
      },
      "source": [
        "In order to apply PCA to the MNIST data we need to reshape the original MNIST data, from\n",
        "\n",
        "    mnist_images.shape == [60000, 28, 28]\n",
        "\n",
        "into a 2d array (matrix)\n",
        "\n",
        "    X_mnist.shape == [60000, 784]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd456e9",
      "metadata": {
        "id": "8dd456e9"
      },
      "outputs": [],
      "source": [
        "# Reshaping\n",
        "mnist_X_original = mnist_X.copy()\n",
        "mnist_X = mnist_X.reshape(len(mnist_X), -1)\n",
        "print(mnist_X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b93f31",
      "metadata": {
        "id": "71b93f31"
      },
      "source": [
        "### Task 3: Apply PCA to the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44615f96",
      "metadata": {
        "id": "44615f96"
      },
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "mnist_X_pca = pca.None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d645557e",
      "metadata": {
        "id": "d645557e"
      },
      "outputs": [],
      "source": [
        "mnist_X_pca.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad1f0b4",
      "metadata": {
        "id": "aad1f0b4"
      },
      "source": [
        "### Task 4: Plot the MNIST data projected onto the first two principal components (using different colours for the different digits). Use the labels to colour the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338836aa",
      "metadata": {
        "id": "338836aa"
      },
      "outputs": [],
      "source": [
        "plot = plt.scatter(mnist_X_pca[:, None], mnist_X_pca[:, None], c=mnist_y, cmap=\"rainbow\") # replace None with correct component..\n",
        "plt.legend(handles=plot.legend_elements()[0], labels=[x for x in range(10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bdc9771",
      "metadata": {
        "id": "0bdc9771"
      },
      "source": [
        "### Task 5: Plot the explained variance per component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23af3e90",
      "metadata": {
        "id": "23af3e90"
      },
      "outputs": [],
      "source": [
        "# Determine explained variance using explained_variance_ratio_ attribute\n",
        "# explained_variance_ratio_\n",
        "#   Percentage of variance explained by each of the selected components.\n",
        "#   If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\n",
        "exp_var_pca = pca.None  # shape: (784,)\n",
        "\n",
        "# Cumulative sum of eigenvalues will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "\n",
        "# Create the plot\n",
        "plt.bar(\n",
        "    range(0, len(exp_var_pca)),\n",
        "    exp_var_pca,\n",
        "    alpha=0.5,\n",
        "    align=\"center\",\n",
        "    label=\"Individual explained variance\",\n",
        ")\n",
        "plt.step(\n",
        "    range(0, len(cum_sum_eigenvalues)),\n",
        "    cum_sum_eigenvalues,\n",
        "    where=\"mid\",\n",
        "    label=\"Cumulative explained variance\",\n",
        ")\n",
        "plt.ylabel(\"Explained variance ratio\")\n",
        "plt.xlabel(\"Principal component index\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe72d87",
      "metadata": {
        "id": "5fe72d87"
      },
      "source": [
        "### Manual way of gettting explained variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71d7f08",
      "metadata": {
        "id": "a71d7f08"
      },
      "outputs": [],
      "source": [
        "def compute_PCA_parameters(X, M):\n",
        "    \"\"\"\n",
        "    This function computes the first M prinicpal components of a\n",
        "    dataset X. It returns the mean of the data, the projection matrix,\n",
        "    and the associated singular values.\n",
        "\n",
        "    While you can compute this however you want, `np.linalg.svd` is\n",
        "    highly recommended. Please look at its documentation to choose\n",
        "    its arguments appropriately, and on how to interpret its return values.\n",
        "\n",
        "    INPUT:\n",
        "    X    : (N, D) matrix; each row is a D-dimensional data point\n",
        "    M    : integer, <= D (number of principal components to return)\n",
        "\n",
        "    OUTPUTS:\n",
        "    x_bar  : (D,) vector, with the mean of the data\n",
        "    W      : (D, M) semi-orthogonal matrix of projections\n",
        "    s      : (D,) vector of singular values\n",
        "    \"\"\"\n",
        "    N, D = X.shape\n",
        "\n",
        "    # center the data\n",
        "    x_bar = np.mean(X, 0) # compute mean across samples\n",
        "    X_bar = X - x_bar # subtract mean\n",
        "\n",
        "    # perform singular value decomposition on centered dataset\n",
        "    u, s, vh = np.linalg.svd(X_bar, full_matrices=False)\n",
        "\n",
        "    # extract principal components and singular values\n",
        "    W = vh.T[:, :M]\n",
        "    return x_bar, W, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "957ad3cb",
      "metadata": {
        "id": "957ad3cb"
      },
      "outputs": [],
      "source": [
        "mnist_mean, W_mnist, s_mnist = compute_PCA_parameters(mnist_X, 50)\n",
        "\n",
        "N, data_dim = mnist_X.shape\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(np.arange(data_dim) + 1, s_mnist**2 / N, \".-\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Explained variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b0ac98",
      "metadata": {
        "id": "60b0ac98"
      },
      "source": [
        "### Task 5: Apply KMeans to cluster the MNIST data\n",
        "\n",
        "- Preprocess the MNIST dataset with PCA to compress it down to a 2-dimensional feature space before applying the K-Means\n",
        "\n",
        "- Try K-Means with different numbers of clusters and use the Silhouette Coefficient to choose the optimal number of cluster\n",
        "\n",
        "Discussion: Does the Silhouette Coefficient chooses the right number of clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67696103",
      "metadata": {
        "id": "67696103"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "mnist_X_pca = pca.fit_transform(mnist_X)\n",
        "n_digits = len(np.unique(mnist_y))\n",
        "print(n_digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40913276",
      "metadata": {
        "id": "40913276"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "kmeans = KMeans(None, n_init=10)\n",
        "kmeans.fit(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a7c0a5",
      "metadata": {
        "id": "49a7c0a5"
      },
      "outputs": [],
      "source": [
        "# Generating the sample data from make_blobs\n",
        "# This particular setting has one distinct cluster and 3 clusters placed close\n",
        "# together.\n",
        "X = None\n",
        "y = None\n",
        "range_n_clusters = [2, 5,10]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters,  n_init=10 , random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(\n",
        "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
        "    )\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(\n",
        "        centers[:, 0],\n",
        "        centers[:, 1],\n",
        "        marker=\"o\",\n",
        "        c=\"white\",\n",
        "        alpha=1,\n",
        "        s=200,\n",
        "        edgecolor=\"k\",\n",
        "    )\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dlmvL3Rw3rff",
      "metadata": {
        "id": "dlmvL3Rw3rff"
      },
      "source": [
        "### Task 6: Apply the K-Means to the MNIST dataset compressed to 2, 10 and 100 PCs (fixing the number of clusters to 10) and quantify the performance of the three cluster models using different clustering metrics\n",
        "\n",
        "See \"Quantifying the quality of clustering results\" in https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db5c4a2",
      "metadata": {
        "id": "1db5c4a2"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from time import time\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "evaluations = []\n",
        "evaluations_std = []\n",
        "labels = mnist_y\n",
        "\n",
        "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
        "    name = km.__class__.__name__ if name is None else name\n",
        "\n",
        "    train_times = []\n",
        "    scores = defaultdict(list)\n",
        "    for seed in range(n_runs):\n",
        "        km.set_params(random_state=seed)\n",
        "        t0 = time()\n",
        "        km.fit(X)\n",
        "        train_times.append(time() - t0)\n",
        "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
        "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
        "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
        "        scores[\"Adjusted Rand-Index\"].append(\n",
        "            metrics.adjusted_rand_score(labels, km.labels_)\n",
        "        )\n",
        "        scores[\"Silhouette Coefficient\"].append(\n",
        "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
        "        )\n",
        "    train_times = np.asarray(train_times)\n",
        "\n",
        "    print(f\"Clustering done in {train_times.mean():.2f} Â± {train_times.std():.2f} s \")\n",
        "    evaluation = {\n",
        "        \"estimator\": name,\n",
        "        \"train_time\": train_times.mean(),\n",
        "    }\n",
        "    evaluation_std = {\n",
        "        \"estimator\": name,\n",
        "        \"train_time\": train_times.std(),\n",
        "    }\n",
        "    for score_name, score_values in scores.items():\n",
        "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
        "        print(f\"{score_name}: {mean_score:.3f} Â± {std_score:.3f}\")\n",
        "        evaluation[score_name] = mean_score\n",
        "        evaluation_std[score_name] = std_score\n",
        "    evaluations.append(evaluation)\n",
        "    evaluations_std.append(evaluation_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fdcb3b",
      "metadata": {
        "id": "82fdcb3b"
      },
      "outputs": [],
      "source": [
        "print(\"------ 2 PCA K-Means ------\")\n",
        "pca = PCA(None)\n",
        "mnist_X_pca = pca.fit_transform(None)\n",
        "kmeans = KMeans(n_clusters=None, n_init=10)\n",
        "fit_and_evaluate(\n",
        "    None\n",
        "    None,\n",
        "    name=\"2-PCA-Kmeans\",\n",
        ")\n",
        "print(\"------ 10 PCA K-Means ------\")\n",
        "pca = PCA(None)\n",
        "mnist_X_pca = pca.fit_transform(None)\n",
        "kmeans = KMeans(None, n_init=10)\n",
        "fit_and_evaluate(\n",
        "    None,\n",
        "    None,\n",
        "    name=\"10-PCA-Kmeans\",\n",
        ")\n",
        "print(\"------ 100 PCA K-Means ------\")\n",
        "pca = PCA(None)\n",
        "mnist_X_pca = pca.fit_transform(None)\n",
        "kmeans = KMeans(None, n_init=10)\n",
        "fit_and_evaluate(\n",
        "    None,\n",
        "    None,\n",
        "    name=\"100-PCA-Kmeans\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309d6168",
      "metadata": {
        "id": "309d6168"
      },
      "outputs": [],
      "source": [
        "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "df = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\n",
        "df_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\n",
        "\n",
        "df.drop(\n",
        "    [\"train_time\"],\n",
        "    axis=\"columns\",\n",
        ").plot.barh(ax=ax0, xerr=df_std)\n",
        "ax0.set_xlabel(\"Clustering scores\")\n",
        "ax0.set_ylabel(\"\")\n",
        "\n",
        "df[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\n",
        "ax1.set_xlabel(\"Clustering time (s)\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V9IFmJy3Lf6K",
      "metadata": {
        "id": "V9IFmJy3Lf6K"
      },
      "source": [
        "### Task 7: Implement another clustering algorithm: hierarchical clustering\n",
        "Example: agglomerative clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tc14xfzjMonq",
      "metadata": {
        "id": "Tc14xfzjMonq"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "mnist_X_pca = pca.fit_transform(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rh-P1HOHNpER",
      "metadata": {
        "id": "rh-P1HOHNpER"
      },
      "outputs": [],
      "source": [
        "# Randomly sample a subset of the data because agglomerative takes a lot of RAM\n",
        "np.random.seed(42)\n",
        "subset_size = None # choose a subset size, for example 300000\n",
        "indices = np.random.choice(mnist_X_pca.shape[0], subset_size, replace=False)\n",
        "X = mnist_X_pca[None]\n",
        "y = mnist_y[None]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dendrogram example**\n",
        "\n",
        "A dendrogram is a tree-like plot that shows how different datapoints get clustered together. Starting at the bottom, where each point is its own cluster, data points get clustered together at each step. As we move up in the dendrogram, the data gets more and more clustered until eventually becoming a single cluster. Clusters linked by a longer branch (longer distance on y axis) are more dissimlar and vice versa.\n",
        "\n",
        "By cutting the dendrogram at a specific height, you can decide how many clusters you want. Cutting at a lower height gives more clusters whereas cutting higher up gives fewer."
      ],
      "metadata": {
        "id": "S5qGypfBjzdP"
      },
      "id": "S5qGypfBjzdP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> To understand the dendrogram better, you can first try with very few samples (e.g. 30) and see how they get clustered together."
      ],
      "metadata": {
        "id": "FU-7NC3hnnsj"
      },
      "id": "FU-7NC3hnnsj"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Generate the linkage matrix for dendrogram\n",
        "linkage_matrix = linkage(None, method='ward')\n",
        "\n",
        "# Plot the dendrogram for the subset\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Hierarchical Clustering Dendrogram (Subset of MNIST)')\n",
        "plt.xlabel('Sample (Digit Label)')\n",
        "plt.ylabel('Distance')\n",
        "\n",
        "# Get the labels corresponding to the subset\n",
        "digit_labels = y\n",
        "\n",
        "# Create dendrogram for the subset\n",
        "dendrogram(\n",
        "    None,\n",
        "    truncate_mode=None,       # Show full dendrogram for the subset\n",
        "    leaf_rotation=90.,        # Rotate x-axis labels\n",
        "    leaf_font_size=10.,       # Font size for x-axis labels\n",
        "    labels=digit_labels       # Use digit labels for x-axis\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IDk6JWyQf7zv"
      },
      "id": "IDk6JWyQf7zv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with fewer samples\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Randomly sample a subset of the data because agglomerative takes a lot of RAM\n",
        "np.random.seed(42)\n",
        "subset_size = 30\n",
        "\n",
        "# For example, take 2000 samples\n",
        "indices = np.random.choice(mnist_X_pca.shape[0], subset_size, replace=False)\n",
        "X = mnist_X_pca[indices]\n",
        "y = mnist_y[indices]\n",
        "\n",
        "# Generate the linkage matrix for dendrogram\n",
        "linkage_matrix = linkage(None, method='ward')\n",
        "\n",
        "# Plot the dendrogram for the subset\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Hierarchical Clustering Dendrogram (Subset of MNIST)')\n",
        "plt.xlabel('Sample (Digit Label)')\n",
        "plt.ylabel('Distance')\n",
        "\n",
        "# Get the labels corresponding to the subset\n",
        "digit_labels = y\n",
        "\n",
        "# Create dendrogram for the subset\n",
        "dendrogram(\n",
        "    None,\n",
        "    truncate_mode=None,       # Show full dendrogram for the subset\n",
        "    leaf_rotation=90.,        # Rotate x-axis labels\n",
        "    leaf_font_size=10.,       # Font size for x-axis labels\n",
        "    labels=digit_labels       # Use digit labels for x-axis\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O6f4WMGBokjL"
      },
      "id": "O6f4WMGBokjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0ca1ef44",
      "metadata": {
        "id": "0ca1ef44"
      },
      "source": [
        "# Part 2: IXI dataset\n",
        "\n",
        "In this part we apply use Gaussian Mixture Model (GMM) to an image segmentation task. We will use the voxel intensities of two brain images to cluster them into four different clusters that should correspond to different types of brain tissue (grey matter, white amtter, Cerebrospinal fluid (CSF) and \"other\").\n",
        "\n",
        "https://scikit-learn.org/stable/modules/mixture.html#gmm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWdM-Op9khoF",
      "metadata": {
        "id": "hWdM-Op9khoF"
      },
      "outputs": [],
      "source": [
        "%pip install matplotlib==3.9.4 nilearn==0.11.0\n",
        "import nibabel\n",
        "import nibabel.processing\n",
        "from nilearn import plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf4e904",
      "metadata": {
        "id": "ebf4e904"
      },
      "source": [
        "### Task 8: Load two brain images (proton density-weighted (PDW) and T2-weighted (T2W)) that have been previously pre-processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a002aab",
      "metadata": {
        "id": "4a002aab"
      },
      "outputs": [],
      "source": [
        "f1 = nibabel.load(\"T2.nii\")\n",
        "f2 = nibabel.load(\"PD.nii\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VEVo7HVk8qAK",
      "metadata": {
        "id": "VEVo7HVk8qAK"
      },
      "source": [
        "Resample image to have all dimensions of equal size (you don't need to worry about this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EvEWFEq_5-9-",
      "metadata": {
        "id": "EvEWFEq_5-9-"
      },
      "outputs": [],
      "source": [
        "f1 =  nibabel.processing.conform(f1, out_shape=(256, 256, 256), voxel_size=(1.0, 1.0, 1.0), order=0, orientation='RAS', out_class=None)\n",
        "f2 =  nibabel.processing.conform(f2, out_shape=(256, 256, 256), voxel_size=(1.0, 1.0, 1.0), order=0, orientation='RAS', out_class=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NbTOdEb_88MN",
      "metadata": {
        "id": "NbTOdEb_88MN"
      },
      "source": [
        "Get images as numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZiA9MB3z8vk6",
      "metadata": {
        "id": "ZiA9MB3z8vk6"
      },
      "outputs": [],
      "source": [
        "x1 = f1.get_fdata()\n",
        "x2 = f2.get_fdata()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eGI1ulFu9Qe-",
      "metadata": {
        "id": "eGI1ulFu9Qe-"
      },
      "source": [
        "Look at images (plots are interactive!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rpeEWKna5pi1",
      "metadata": {
        "id": "rpeEWKna5pi1"
      },
      "outputs": [],
      "source": [
        "plotting.view_img(f1, bg_img=False, black_bg=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIqibfBP9PxA",
      "metadata": {
        "id": "IIqibfBP9PxA"
      },
      "outputs": [],
      "source": [
        "plotting.view_img(f2, bg_img=False, black_bg=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fc247e",
      "metadata": {
        "id": "90fc247e"
      },
      "source": [
        "### Task 9: Vectorize the brain images and combine them to create a data matrix with 2 dimensions per voxel (PD and T2 intensities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914ab5dc",
      "metadata": {
        "id": "914ab5dc"
      },
      "outputs": [],
      "source": [
        "# Flatten the 3D arrays to 2D arrays where each row is a voxel\n",
        "x1_flat = x1.reshape(-1, 1) # Reshape x1 to have one column and as many rows as there are voxels\n",
        "x2_flat = x2.reshape(-1, 1) # Reshape x2 similarly\n",
        "\n",
        "# Combine the flattened arrays side by side to create a new 2D array with two columns (PD and T2 intensities)\n",
        "data_matrix = np.hstack(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_60o68S5jmPg",
      "metadata": {
        "id": "_60o68S5jmPg"
      },
      "outputs": [],
      "source": [
        "print(data_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Igqa6F4jUkWc",
      "metadata": {
        "id": "Igqa6F4jUkWc"
      },
      "outputs": [],
      "source": [
        "# reshaping to original\n",
        "original_shape = x1.shape\n",
        "\n",
        "# Split the 2D data_matrix back into two 1D arrays\n",
        "x_1_flat = data_matrix[:, 0]  # This extracts the first column (PD intensities)\n",
        "x_2_flat = data_matrix[:, 1]  # This extracts the second column (T2 intensities)\n",
        "\n",
        "# Reshape these 1D arrays back into their original 3D shape\n",
        "x1_reshaped = x_1_flat.reshape(None)\n",
        "x2_reshaped = x_2_flat.reshape(None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SHxgVnJSuScu",
      "metadata": {
        "id": "SHxgVnJSuScu"
      },
      "outputs": [],
      "source": [
        "print(x_1_flat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UfWMDxcyUm9W",
      "metadata": {
        "id": "UfWMDxcyUm9W"
      },
      "outputs": [],
      "source": [
        "# check if original and un-reshape reshaped match\n",
        "print(np.array_equal(x1, x1_reshaped))\n",
        "print(np.array_equal(x2, x2_reshaped))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7c43e2",
      "metadata": {
        "id": "8b7c43e2"
      },
      "source": [
        "###  Task 10: Apply GMM to cluster the voxels fixing the number of cluster to 4\n",
        "We expect the four classes to correspond to:\n",
        "- grey matter\n",
        "- white matter\n",
        "- cerebrospinal fluid\n",
        "- other (background)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the GMM for desired number of clusters"
      ],
      "metadata": {
        "id": "x5bVmvGe8-Q_"
      },
      "id": "x5bVmvGe8-Q_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a605147b",
      "metadata": {
        "id": "a605147b"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Assuming data_matrix is the combined 2D array from the previous steps\n",
        "\n",
        "# Initialize the Gaussian Mixture Model with 4 components (clusters)\n",
        "gmm = GaussianMixture(None, random_state=0)\n",
        "\n",
        "# Fit the model to the data and predict the cluster for each voxel\n",
        "cluster_labels = gmm.fit_predict(None)\n",
        "\n",
        "# cluster_labels is now a 1D array with the cluster label (0, 1, 2, 3) for each voxel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sYrGJdn6jyqy",
      "metadata": {
        "id": "sYrGJdn6jyqy"
      },
      "outputs": [],
      "source": [
        "print(cluster_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viusalise the clusters"
      ],
      "metadata": {
        "id": "EK48sFQW83_6"
      },
      "id": "EK48sFQW83_6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d271a152",
      "metadata": {
        "id": "d271a152"
      },
      "outputs": [],
      "source": [
        "# For visualisation purposes add 1 to each cluster (to avoid confusion with background - although here background is a cluster so this will make no difference)\n",
        "cluster_labels += 1\n",
        "\n",
        "# Reshape cluster_labels back to the original 3D shape of the images\n",
        "clustered_image = cluster_labels.reshape(None)\n",
        "\n",
        "# clustered_image is a 3D array with the same shape as the original images,\n",
        "# where each voxel's value represents its cluster label.\n",
        "print(clustered_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fBcbe0cjMUF",
      "metadata": {
        "id": "1fBcbe0cjMUF"
      },
      "outputs": [],
      "source": [
        "# Convert the clustered image to a Nifti1Image object using the correct affine matrix\n",
        "clustered_img_nii = nibabel.Nifti1Image(clustered_image.astype(np.int16), affine=f1.affine)\n",
        "\n",
        "# View the clustered image in an interactive viewer\n",
        "plotting.view_img(None, threshold='auto', cmap='magma', symmetric_cmap=False, vmin=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WXGdTTabFEBN",
      "metadata": {
        "id": "WXGdTTabFEBN"
      },
      "source": [
        "> **Question**: do you recognise the four classes? Do they correspond to the brain structures identified earlier?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cf5fdd",
      "metadata": {
        "id": "75cf5fdd"
      },
      "source": [
        "### Task 11: Convert the class probabilities for each voxel to a 3D image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JclJoAyDxtys",
      "metadata": {
        "id": "JclJoAyDxtys"
      },
      "source": [
        "Alternative, instead of classes, based on probabilities.\n",
        "<br> First, compute the cluster probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9OZhy_xRtUMm",
      "metadata": {
        "id": "9OZhy_xRtUMm"
      },
      "outputs": [],
      "source": [
        "# Predict the probabilities for each voxel belonging to each cluster\n",
        "voxel_probabilities = gmm.predict_proba(None)\n",
        "\n",
        "# Now, voxel_probabilities is a 2D numpy array where each row represents a voxel\n",
        "# and each column represents the probability of that voxel belonging to a certain cluster.\n",
        "\n",
        "# Reshape these probabilities back to the original 3D shape for each cluster\n",
        "probabilities_4d = np.zeros((x1.shape[0], x1.shape[1], x1.shape[2], gmm.n_components))\n",
        "\n",
        "for i in range(gmm.n_components):\n",
        "    # Reshape the probabilities for cluster i into the original 3D shape\n",
        "    probabilities_4d[:,:,:,i] = voxel_probabilities[:,i].reshape(x1.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the individual clusters"
      ],
      "metadata": {
        "id": "tcV_wU0D8xmK"
      },
      "id": "tcV_wU0D8xmK"
    },
    {
      "cell_type": "markdown",
      "id": "zvYOIvpWFRT9",
      "metadata": {
        "id": "zvYOIvpWFRT9"
      },
      "source": [
        "**Question:** Which brain structure do each of these correspond to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qy9fLCYowcZO",
      "metadata": {
        "id": "qy9fLCYowcZO"
      },
      "outputs": [],
      "source": [
        "# Convert the probability maps to NIfTI images and visualize\n",
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster 0 Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MVWpms6dHkcq",
      "metadata": {
        "id": "MVWpms6dHkcq"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster 1 Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-kckHQJDxj0q",
      "metadata": {
        "id": "-kckHQJDxj0q"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster 2 Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ElqJWP1oyXCQ",
      "metadata": {
        "id": "ElqJWP1oyXCQ"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster 3 Probability Map')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L2eyRGZPpH-_",
      "metadata": {
        "id": "L2eyRGZPpH-_"
      },
      "source": [
        "### Task 11: Use three clusters only. We want to ignore the background.\n",
        "\n",
        "We now expect the following clusters:\n",
        "- grey matter\n",
        "- white matter\n",
        "- cerebrospinal fluid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X7HbTa2bJFtH",
      "metadata": {
        "id": "X7HbTa2bJFtH"
      },
      "source": [
        "Create and apply a mask to filter out the background. We use the same data matrix as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SwNY_ezjJJLF",
      "metadata": {
        "id": "SwNY_ezjJJLF"
      },
      "outputs": [],
      "source": [
        "# Mask for non-zero pixels\n",
        "non_zero_mask = None\n",
        "\n",
        "# Filter out the zero-valued pixels\n",
        "data_matrix_non_zero = data_matrix[None]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0--nS64aJKiS",
      "metadata": {
        "id": "0--nS64aJKiS"
      },
      "source": [
        "Apply GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UVAxMURJJMXH",
      "metadata": {
        "id": "UVAxMURJJMXH"
      },
      "outputs": [],
      "source": [
        "# Initialize the Gaussian Mixture Model\n",
        "gmm = GaussianMixture(None, random_state=0)\n",
        "\n",
        "cluster_labels_non_zero = gmm.fit_predict(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "luIKN1PrJhAR",
      "metadata": {
        "id": "luIKN1PrJhAR"
      },
      "source": [
        "Plot the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qn9oUt7UxqjK",
      "metadata": {
        "id": "Qn9oUt7UxqjK"
      },
      "outputs": [],
      "source": [
        "# Adjust the labels to start from 1 instead of 0 (for visualisation purposes)\n",
        "cluster_labels_non_zero += 1\n",
        "\n",
        "# Initialize an output array with zeros for the background\n",
        "clustered_image = np.zeros(x1_flat.shape[0], dtype=np.int16)\n",
        "\n",
        "# Assign the GMM cluster labels back to the non-zero pixels in the output array\n",
        "clustered_image[non_zero_mask] = cluster_labels_non_zero\n",
        "\n",
        "# Reshape cluster_labels back to the original 3D shape of the images\n",
        "clustered_image_reshaped = clustered_image.reshape(None)\n",
        "\n",
        "# Convert the clustered image to a Nifti1Image object using the correct affine matrix\n",
        "clustered_img_nii = nibabel.Nifti1Image(clustered_image_reshaped, affine=f1.affine)\n",
        "\n",
        "# View the clustered image\n",
        "plotting.view_img(clustered_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ruvtlhxhJjPq",
      "metadata": {
        "id": "ruvtlhxhJjPq"
      },
      "source": [
        "### Task 12: Look at individual probabilites for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the cluster probabilities"
      ],
      "metadata": {
        "id": "xrHiTfwe8M2c"
      },
      "id": "xrHiTfwe8M2c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5qrV7OEIpVDh",
      "metadata": {
        "id": "5qrV7OEIpVDh"
      },
      "outputs": [],
      "source": [
        "# Predict the probabilities for each voxel belonging to each cluster\n",
        "voxel_probabilities = gmm.predict_proba(None)\n",
        "\n",
        "# Initialize an output 4D array for probabilities with zeros\n",
        "# This array has the same width, height, and depth as the original images, and an extra dimension for each cluster\n",
        "probabilities_4d = np.zeros((x1.shape[0], x1.shape[1], x1.shape[2], gmm.n_components))\n",
        "\n",
        "# We need a way to map the probabilities back to their original positions including zeros\n",
        "# First, flatten the 4D array to match the shape of the non-zero processing (x1_flat.shape[0], n_components)\n",
        "probabilities_flat = probabilities_4d.reshape(-1, gmm.n_components)\n",
        "\n",
        "# Now, assign the probabilities back to the non-zero positions in the flattened array\n",
        "probabilities_flat[non_zero_mask, :] = voxel_probabilities\n",
        "\n",
        "# Finally, reshape this flat probabilities array back to the original 4D shape\n",
        "probabilities_4d = probabilities_flat.reshape(x1.shape[0], x1.shape[1], x1.shape[2], gmm.n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now visualise the clusters"
      ],
      "metadata": {
        "id": "7tn1aj_O8Kb5"
      },
      "id": "7tn1aj_O8Kb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7nI2IAfph6A",
      "metadata": {
        "id": "i7nI2IAfph6A"
      },
      "outputs": [],
      "source": [
        "# Convert the probability maps to NIfTI images and visualize\n",
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster {0} Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iI2Ia2z6pkaR",
      "metadata": {
        "id": "iI2Ia2z6pkaR"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster {1} Probability Map')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t-w3q9RzpnCN",
      "metadata": {
        "id": "t-w3q9RzpnCN"
      },
      "outputs": [],
      "source": [
        "prob_img_nii = nibabel.Nifti1Image(probabilities_4d[:,:,:,None].astype(np.float32), affine=f1.affine)\n",
        "plotting.view_img(prob_img_nii,\n",
        "                  threshold='auto', bg_img=False, black_bg=True, cmap='magma', symmetric_cmap=False, vmin=0, title=f'Cluster {2} Probability Map')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}